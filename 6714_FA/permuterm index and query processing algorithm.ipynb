{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'Healco', ',', 'Heato', ',', 'Ahelt', ',', 'this', 'is', 'a', 'test', 'corpus', ':', 'Corpus', 'linguistics', 'proposes', 'that', 'reliable', 'language', 'analysis', 'is', 'more', 'feasible', 'with', 'corpora', 'collected', 'in', 'the', 'field', ',', 'in', 'their', 'natural', 'contexts', ',', 'and', 'with', 'minimal', 'experimental-interference', '.']\n",
      "['hello', 'healco', 'heato', 'ahelt', 'this', 'is', 'a', 'test', 'corpus', 'corpus', 'linguistics', 'proposes', 'that', 'reliable', 'language', 'analysis', 'is', 'more', 'feasible', 'with', 'corpora', 'collected', 'in', 'the', 'field', 'in', 'their', 'natural', 'contexts', 'and', 'with', 'minimal']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "raw1 = 'Hello,Healco,Heato,Ahelt,this is a test corpus:Corpus linguistics proposes that reliable language analysis is more feasible with corpora collected in the field, in their natural contexts, and with minimal experimental-interference.'\n",
    "tokens = nltk.word_tokenize(raw1)\n",
    "print(tokens)\n",
    "## change tokens to lowercase\n",
    "tokens=[word.lower() for word in tokens if word.isalpha()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permuterm index\n",
    "Now we define the method to generate Permuterm index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Generate_Permuterm_index(raw):\n",
    "    Permuterm_index = []\n",
    "    for token in raw:\n",
    "        index = ''\n",
    "        cursor = 0\n",
    "        index = '$' + token\n",
    "        Permuterm_index.append(index)\n",
    "        while cursor <= len(token):\n",
    "            index = ''\n",
    "            if cursor == 0:\n",
    "                cursor += 1\n",
    "                continue\n",
    "            else:\n",
    "                index = token[-cursor:] + '$' + token[:-cursor]\n",
    "            Permuterm_index.append(index)            \n",
    "            cursor += 1        \n",
    "    return Permuterm_index        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Generate_Permuterm_index method using a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$hello', 'o$hell', 'lo$hel', 'llo$he', 'ello$h', 'hello$', '$healco', 'o$healc', 'co$heal', 'lco$hea', 'alco$he', 'ealco$h', 'healco$', '$heato', 'o$heat', 'to$hea', 'ato$he', 'eato$h', 'heato$', '$ahelt', 't$ahel', 'lt$ahe', 'elt$ah', 'helt$a', 'ahelt$', '$this', 's$thi', 'is$th', 'his$t', 'this$', '$is', 's$i', 'is$', '$a', 'a$', '$test', 't$tes', 'st$te', 'est$t', 'test$', '$corpus', 's$corpu', 'us$corp', 'pus$cor', 'rpus$co', 'orpus$c', 'corpus$', '$corpus', 's$corpu', 'us$corp', 'pus$cor', 'rpus$co', 'orpus$c', 'corpus$', '$linguistics', 's$linguistic', 'cs$linguisti', 'ics$linguist', 'tics$linguis', 'stics$lingui', 'istics$lingu', 'uistics$ling', 'guistics$lin', 'nguistics$li', 'inguistics$l', 'linguistics$', '$proposes', 's$propose', 'es$propos', 'ses$propo', 'oses$prop', 'poses$pro', 'oposes$pr', 'roposes$p', 'proposes$', '$that', 't$tha', 'at$th', 'hat$t', 'that$', '$reliable', 'e$reliabl', 'le$reliab', 'ble$relia', 'able$reli', 'iable$rel', 'liable$re', 'eliable$r', 'reliable$', '$language', 'e$languag', 'ge$langua', 'age$langu', 'uage$lang', 'guage$lan', 'nguage$la', 'anguage$l', 'language$', '$analysis', 's$analysi', 'is$analys', 'sis$analy', 'ysis$anal', 'lysis$ana', 'alysis$an', 'nalysis$a', 'analysis$', '$is', 's$i', 'is$', '$more', 'e$mor', 're$mo', 'ore$m', 'more$', '$feasible', 'e$feasibl', 'le$feasib', 'ble$feasi', 'ible$feas', 'sible$fea', 'asible$fe', 'easible$f', 'feasible$', '$with', 'h$wit', 'th$wi', 'ith$w', 'with$', '$corpora', 'a$corpor', 'ra$corpo', 'ora$corp', 'pora$cor', 'rpora$co', 'orpora$c', 'corpora$', '$collected', 'd$collecte', 'ed$collect', 'ted$collec', 'cted$colle', 'ected$coll', 'lected$col', 'llected$co', 'ollected$c', 'collected$', '$in', 'n$i', 'in$', '$the', 'e$th', 'he$t', 'the$', '$field', 'd$fiel', 'ld$fie', 'eld$fi', 'ield$f', 'field$', '$in', 'n$i', 'in$', '$their', 'r$thei', 'ir$the', 'eir$th', 'heir$t', 'their$', '$natural', 'l$natura', 'al$natur', 'ral$natu', 'ural$nat', 'tural$na', 'atural$n', 'natural$', '$contexts', 's$context', 'ts$contex', 'xts$conte', 'exts$cont', 'texts$con', 'ntexts$co', 'ontexts$c', 'contexts$', '$and', 'd$an', 'nd$a', 'and$', '$with', 'h$wit', 'th$wi', 'ith$w', 'with$', '$minimal', 'l$minima', 'al$minim', 'mal$mini', 'imal$min', 'nimal$mi', 'inimal$m', 'minimal$']\n"
     ]
    }
   ],
   "source": [
    "permuterm = Generate_Permuterm_index(tokens)\n",
    "print(Generate_Permuterm_index(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The query processing algorithm\n",
    "The query processing algorithm takes a query containing * and return the answer. Note that there could be more than 1 * in the query."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For query like fi*mo*er. We first enumerate the terms in the dictionary that are in the permuterm index of er$fi*. Not all such dictionary terms will have the string mo in the middle - we filter these out by exhaustive enumeration, checking each candidate to see if it contains mo. In this example, the term fishmonger would survive this filtering but filibuster would not. We then run the surviving terms through the standard inverted index for document retrieval.(http://nlp.stanford.edu/IR-book/html/htmledition/permuterm-indexes-1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wildcard_query(query,permuterm_index):\n",
    "    count = 0\n",
    "    ## calculate tha amount of wildcard in query so that we can consider separately\n",
    "    for symbol in query:\n",
    "        if symbol == '*':\n",
    "            count += 1\n",
    "    result = []\n",
    "    ## if the amount of wildcard equals to 1\n",
    "    if count == 1:\n",
    "        splited_query = query.split('*')\n",
    "        if query[-1] == '*':        \n",
    "            for term in permuterm_index:\n",
    "                pattern = term\n",
    "                splited_parts = pattern.split('$')\n",
    "                if splited_parts[1] == splited_query[0]:                \n",
    "                    result.append(splited_parts[1] + splited_parts[0])\n",
    "        elif query[0] == '*':\n",
    "            for term in permuterm_index:\n",
    "                pattern = term\n",
    "                splited_parts = pattern.split('$')\n",
    "                if splited_parts[0] == splited_query[1]:\n",
    "                    result.append(splited_parts[1] + splited_parts[0])\n",
    "        elif query[0] != '*' and query [-1] != '*':\n",
    "            suffix_length = len(splited_query[0])\n",
    "            for term in permuterm_index:\n",
    "                pattern = term\n",
    "                splited_parts = pattern.split('$')\n",
    "                if suffix_length > len(splited_parts[1]):\n",
    "                    continue\n",
    "                suffix = splited_parts[1][:suffix_length]\n",
    "                if splited_parts[0] == splited_query[1] and splited_query[0] == suffix:\n",
    "                    result.append(splited_parts[1] + splited_parts[0])\n",
    "    ## if the amount of wildcard is larger than 1\n",
    "    elif count > 1:\n",
    "        temp_result = []\n",
    "        splited_query = query.split('*')\n",
    "        suffix_length = len(splited_query[0])\n",
    "        length = len(splited_query[2])\n",
    "        ## the wildcards locates in the middle of query word\n",
    "        if query[0] != '*' and query [-1] != '*':\n",
    "            for term in permuterm_index:\n",
    "                pattern = term\n",
    "                splited_parts = pattern.split('$')\n",
    "                if suffix_length > len(splited_parts[1]):\n",
    "                    continue\n",
    "                suffix = splited_parts[1][:suffix_length]\n",
    "                if splited_parts[0] == splited_query[2] and splited_query[0] == suffix:\n",
    "                    temp_result.append(splited_parts[1] + splited_parts[0])\n",
    "            for term in temp_result:\n",
    "                if splited_query[1] in term[suffix_length:-length]:\n",
    "                    result.append(term)\n",
    "        ## the heading and trailing characters are wildcard\n",
    "        elif query[0] == '*' and query [-1] == '*': \n",
    "            suffix_length = len(splited_query[1])\n",
    "            for term in permuterm_index:\n",
    "                pattern = term\n",
    "                splited_parts = pattern.split('$')\n",
    "                if splited_query[1] in splited_parts[1]:                \n",
    "                    result.append(splited_parts[1] + splited_parts[0])\n",
    "            result = list(set(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the wildcard query(Notice we support the query containing 2 wildcard at most). Our corpus is 'Hello,Healco,Heato,Ahelt,this is a test corpus:Corpus linguistics proposes that reliable language analysis is more feasible with corpora collected in the field, in their natural contexts, and with minimal experimental-interference.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corpus', 'corpus', 'corpora']\n",
      "['natural', 'minimal']\n",
      "['hello']\n",
      "['hello', 'healco']\n",
      "['ahelt', 'hello']\n"
     ]
    }
   ],
   "source": [
    "print(wildcard_query('cor*',permuterm))\n",
    "print(wildcard_query('*al',permuterm))\n",
    "print(wildcard_query('hel*o',permuterm))\n",
    "print(wildcard_query('he*l*o',permuterm))\n",
    "print(wildcard_query('*hel*',permuterm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
